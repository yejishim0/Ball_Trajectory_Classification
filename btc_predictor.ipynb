{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5281a2be",
   "metadata": {},
   "source": [
    "## Load Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e5cc137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the trained classifier from the file\n",
    "BTC_SAVED_PREDICTOR_FILEPATH = './trained/btc_saved_predictor.pkl'\n",
    "classifier = joblib.load(BTC_SAVED_PREDICTOR_FILEPATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cfbc41",
   "metadata": {},
   "source": [
    "## Extract input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47a8facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from btc_feature_extractor import btc_extract_features\n",
    "\n",
    "# Define your input and result file paths\n",
    "input_file_path = './working/tracked_coordinates_challengeset_video.csv'\n",
    "result_file_path = './working/features_challengeset.csv'\n",
    "# Call the btc_extract_feature function\n",
    "btc_extract_features(input_file_path, result_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05fd619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "X = []\n",
    "\n",
    "# Load data from the result_file_path\n",
    "with open(result_file_path, 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    headers = next(csv_reader)  # Skip the header row\n",
    "\n",
    "    for row in csv_reader:\n",
    "        x_data = [float(row[0]), float(row[1]), float(row[2]), float(row[3])]\n",
    "        X.append(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e5673f",
   "metadata": {},
   "source": [
    "## Load scaler and Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96f9b4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_X dimension is (5968, 4)\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 저장된 정규화 매개변수를 로드합니다.\n",
    "normalizer_filename = './trained/btc_saved_scaler.pkl'\n",
    "loaded_mean, loaded_scale = joblib.load(normalizer_filename)\n",
    "\n",
    "# 로드한 매개변수를 사용하여 StandardScaler 객체를 생성합니다.\n",
    "loaded_normalizer = StandardScaler()\n",
    "loaded_normalizer.mean_ = loaded_mean\n",
    "loaded_normalizer.scale_ = loaded_scale\n",
    "\n",
    "# 테스트 데이터에 로드한 정규화 매개변수를 적용하여 정규화합니다.\n",
    "normalized_X = loaded_normalizer.transform(X)\n",
    "\n",
    "print(f'normalized_X dimension is {normalized_X.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae2969c",
   "metadata": {},
   "source": [
    "### Save normalized feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc377a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Define the file path for the normalized_features.csv file\n",
    "normalized_features_file_path = 'normalized_Xtest_features_challengeset.csv'\n",
    "\n",
    "# Write the X_test data to the normalized_features.csv file\n",
    "with open(normalized_features_file_path, 'w', newline='') as normalized_features_file:\n",
    "    csv_writer = csv.writer(normalized_features_file)\n",
    "    \n",
    "    # Write the header row\n",
    "    csv_writer.writerow(['Start Acceleration Y', 'End Acceleration Y', 'Max Jerk Y', 'Min Jerk Y'])\n",
    "    \n",
    "    # Write the data\n",
    "    for x_data in normalized_X:\n",
    "        csv_writer.writerow(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5782b02a",
   "metadata": {},
   "source": [
    "## Predict with predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c68264f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5968\n",
      "['L' 'L' 'P' ... 'L' 'P' 'L']\n"
     ]
    }
   ],
   "source": [
    "# use the loaded classifier to make predictions on new data\n",
    "new_pred = classifier.predict(normalized_X) \n",
    "print(len(new_pred))\n",
    "print(new_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78e1f90",
   "metadata": {},
   "source": [
    "### Save predicted trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8838331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "# Define the file path for the btc_prediction.csv file\n",
    "prediction_file_path = 'btc_prediction.csv'\n",
    "\n",
    "# Write the predictions (new_pred) to the btc_prediction.csv file\n",
    "with open(prediction_file_path, 'w', newline='') as prediction_file:\n",
    "    csv_writer = csv.writer(prediction_file)\n",
    "    \n",
    "    # Write the header row\n",
    "    csv_writer.writerow(['Prediction'])\n",
    "    \n",
    "    # Write the predictions\n",
    "    for prediction in new_pred:\n",
    "        csv_writer.writerow([prediction])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b07b33eba701f3f0bf540f3975da6a802d009916c03a7256fa5e12b717178d60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
