{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "268227f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/input/annotations.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yc/jgk0s2jn24b9gj0fqybsm39w0000gn/T/ipykernel_16750/623641902.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/input/annotations.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mcsv_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_reader\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Skip the header row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/input/annotations.csv'"
     ]
    }
   ],
   "source": [
    "pattern = ''  # Initialize the pattern variable\n",
    "\n",
    "import csv\n",
    "with open('/input/annotations.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    next(csv_reader)  # Skip the header row\n",
    "    for row in csv_reader:\n",
    "        label = row[1]\n",
    "        if label == 'linear':\n",
    "            pattern += 'L'\n",
    "        elif label == 'parabolic':\n",
    "            pattern += 'P'\n",
    "        elif label == 'collision':\n",
    "            pattern += 'C'\n",
    "#print(len(pattern))\n",
    "#print(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9efcfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_size = 30\n",
    "classifications = []\n",
    "\n",
    "# Classify each group\n",
    "for i in range(len(pattern) - group_size + 1):\n",
    "    group = pattern[i:i + group_size]\n",
    "    l_count = group.count('L')\n",
    "    c_count = group.count('C')\n",
    "    p_count = group.count('P')\n",
    "\n",
    "    if l_count == group_size:\n",
    "        classifications.append('L')\n",
    "    elif p_count == group_size:\n",
    "        classifications.append('P')\n",
    "    elif l_count == group_size - 1 and c_count == 1:\n",
    "        classifications.append('L')\n",
    "    elif p_count == group_size - 1 and c_count == 1:\n",
    "        classifications.append('P')\n",
    "    elif l_count >= 1 and c_count >= 1 and p_count >= 1:\n",
    "        if 'L' in group[:group_size - 2] and 'P' in group[group_size - 1:]:\n",
    "            classifications.append('LP')\n",
    "        else:\n",
    "            classifications.append('PL')\n",
    "    elif l_count >= 28 and c_count >= 1 and p_count == 0:\n",
    "        classifications.append('LL')\n",
    "    elif p_count >= 1 + c_count >= 1 + l_count >= 1:\n",
    "        classifications.append('PL')\n",
    "    elif c_count == group_size:\n",
    "        classifications.append('PP')\n",
    "    else:\n",
    "        classifications.append('U')\n",
    "\n",
    "    #print(group, classifications[i])\n",
    "\n",
    "#print(len(classifications))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9880719",
   "metadata": {},
   "source": [
    "## Feature extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d09c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "window_size = 30\n",
    "\n",
    "moving_averages = []  \n",
    "\n",
    "positions = []\n",
    "velocities = []\n",
    "accelerations = []\n",
    "jerks = []\n",
    "\n",
    "max_velocity_y = []\n",
    "min_velocity_y = []\n",
    "max_acceleration_y = []\n",
    "min_acceleration_y = []\n",
    "start_accelerations_y = []\n",
    "end_accelerations_y = []\n",
    "max_jerk_y = []\n",
    "min_jerk_y = []\n",
    "\n",
    "with open('/Users/sim-yeji/Desktop/BTC/input/original_coordinates.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    headers = next(csv_reader)  # Skip the header row\n",
    "\n",
    "    prev_frame_number = None\n",
    "    prev_position_x = None\n",
    "    prev_position_y = None\n",
    "    prev_velocity_x = None\n",
    "    prev_velocity_y = None\n",
    "    acceleration_x = 0.0  # Initialize acceleration_x\n",
    "    acceleration_y = 0.0  # Initialize acceleration_y\n",
    "    prev_acceleration_x = None\n",
    "    prev_acceleration_y = None\n",
    "    prev_jerk_x = None\n",
    "    prev_jerk_y = None\n",
    "    jerk_x = 0.0  # Initialize jerk_x\n",
    "    jerk_y = 0.0  # Initialize jerk_y\n",
    "\n",
    "    for row in csv_reader:\n",
    "        frame_number = int(row[0])\n",
    "        position_x = float(row[1])\n",
    "        position_y = float(row[2])\n",
    "\n",
    "        velocity_x = 0.0  # Initialize velocity_x\n",
    "        velocity_y = 0.0  # Initialize velocity_y\n",
    "\n",
    "        if prev_frame_number is not None:\n",
    "            time_interval = frame_number - prev_frame_number\n",
    "\n",
    "            if time_interval != 0:\n",
    "                velocity_x = (position_x - prev_position_x) / time_interval\n",
    "                velocity_y = (position_y - prev_position_y) / time_interval\n",
    "                velocities.append((frame_number, velocity_x, velocity_y))\n",
    "\n",
    "                acceleration_x = (velocity_x - prev_velocity_x) / time_interval\n",
    "                acceleration_y = (velocity_y - prev_velocity_y) / time_interval\n",
    "                accelerations.append((frame_number, acceleration_x, acceleration_y))\n",
    "\n",
    "                jerk_x = (acceleration_x - prev_acceleration_x) / time_interval\n",
    "                jerk_y = (acceleration_y - prev_acceleration_y) / time_interval\n",
    "                jerks.append((frame_number, jerk_x, jerk_y))\n",
    "\n",
    "        prev_frame_number = frame_number\n",
    "        prev_position_x = position_x\n",
    "        prev_position_y = position_y\n",
    "        prev_velocity_x = velocity_x\n",
    "        prev_velocity_y = velocity_y\n",
    "        prev_acceleration_x = acceleration_x\n",
    "        prev_acceleration_y = acceleration_y\n",
    "        prev_jerk_x = jerk_x\n",
    "        prev_jerk_y = jerk_y\n",
    "        \n",
    "        if len(accelerations) >= 30:\n",
    "            group_velocities = velocities[-30:]\n",
    "            max_velocity_y_value = max(group_velocities, key=lambda x: x[2])[2]\n",
    "            max_velocity_y.append(max_velocity_y_value)\n",
    "            \n",
    "            min_velocity_y_value = min(group_velocities, key=lambda x: x[2])[2]\n",
    "            min_velocity_y.append(min_velocity_y_value)\n",
    "            \n",
    "            group_accelerations = accelerations[-30:]\n",
    "            max_acceleration_y_value = max(group_accelerations, key=lambda x: x[2])[2]\n",
    "            max_acceleration_y.append(max_acceleration_y_value)\n",
    "            \n",
    "            start_accelerations_y_value = group_accelerations[0][2]\n",
    "            start_accelerations_y.append(start_accelerations_y_value)\n",
    "            \n",
    "            end_accelerations_y_value = group_accelerations[-1][2]\n",
    "            end_accelerations_y.append(end_accelerations_y_value)\n",
    "            \n",
    "            min_acceleration_y_value = min(group_accelerations, key=lambda x: x[2])[2]\n",
    "            min_acceleration_y.append(min_acceleration_y_value)\n",
    "            \n",
    "            group_jerks = jerks[-30:]\n",
    "            max_jerk_y_value = max(group_jerks, key=lambda x: x[2])[2]\n",
    "            max_jerk_y.append(max_jerk_y_value)\n",
    "            \n",
    "            min_jerk_y_value = min(group_jerks, key=lambda x: x[2])[2]\n",
    "            min_jerk_y.append(min_jerk_y_value)\n",
    "            \n",
    "        if len(accelerations) >= window_size:\n",
    "            acceleration_window = accelerations[-window_size:]\n",
    "            avg_acceleration_y = sum(item[2] for item in acceleration_window) / window_size\n",
    "\n",
    "            moving_averages.append(avg_acceleration_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4992316",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1724d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in range(len(max_acceleration_y)):\n",
    "    x_data = [start_accelerations_y[i], end_accelerations_y[i], max_jerk_y[i], min_jerk_y[i]]\n",
    "    X.append(x_data)\n",
    "print(len(X)) \n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbbdc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for i in range(len(classifications)):\n",
    "    y.append(classifications[i])\n",
    "    \n",
    "y = y[:len(X)]\n",
    "\n",
    "print(len(y))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821beeae",
   "metadata": {},
   "source": [
    "## Build scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d303f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7956344d",
   "metadata": {},
   "source": [
    "### Save scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff63be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# 저장할 파일 이름\n",
    "normalizer_filename = 'btc_saved_scaler.pkl'\n",
    "\n",
    "# StandardScaler 객체를 생성하고 훈련 데이터에 맞게 fit_transform을 수행합니다.\n",
    "normalizer = StandardScaler()\n",
    "X_train = normalizer.fit_transform(X_train)\n",
    "\n",
    "# 훈련 데이터에 대한 정규화 매개변수를 저장합니다.\n",
    "joblib.dump((normalizer.mean_, normalizer.scale_), normalizer_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410bbf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 저장된 정규화 매개변수를 로드합니다.\n",
    "normalizer_filename = 'btc_saved_scaler.pkl'\n",
    "loaded_mean, loaded_scale = joblib.load(normalizer_filename)\n",
    "\n",
    "# 로드한 매개변수를 사용하여 StandardScaler 객체를 생성합니다.\n",
    "loaded_normalizer = StandardScaler()\n",
    "loaded_normalizer.mean_ = loaded_mean\n",
    "loaded_normalizer.scale_ = loaded_scale\n",
    "\n",
    "# 테스트 데이터에 로드한 정규화 매개변수를 적용하여 정규화합니다.\n",
    "X_test = loaded_normalizer.transform(X_test)\n",
    "\n",
    "print(f'Train set dimension is {X_train.shape}')\n",
    "print(f'Test set dimension is {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc6d8dd",
   "metadata": {},
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2939d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa385a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c164e9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54ddcb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8947dc92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fc9daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a56f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
